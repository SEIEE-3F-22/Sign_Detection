{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a419da6c-c11c-47a3-bd6a-48797787e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['./runs/train/exp/weights/best.pt'], imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['torchscript', 'onnx']\n",
      "YOLOv5 ðŸš€ v6.1-11-g63ddb6f torch 1.10.0+cu113 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients, 16.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/train/exp/weights/best.pt with output shape (1, 25200, 85) (14.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 1.10.0+cu113...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success, saved as runs/train/exp/weights/best.torchscript (29.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.11.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success, saved as runs/train/exp/weights/best.onnx (29.3 MB)\n",
      "\n",
      "Export complete (37.43s)\n",
      "Results saved to \u001b[1m/root/autodl-tmp/yolov5/runs/train/exp/weights\u001b[0m\n",
      "Detect:          python detect.py --weights runs/train/exp/weights/best.onnx\n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train/exp/weights/best.onnx')\n",
      "Validate:        python val.py --weights runs/train/exp/weights/best.onnx\n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights ./runs/train/exp/weights/best.pt --img 640 --batch 1  # export at 640x640 with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5596386d-5133-4eaf-894d-b7954fb518bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./runs/train/exp/weights/best.onnx'], source=./data/images/bus.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=onnx, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.1-11-g63ddb6f torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "Loading runs/train/exp/weights/best.onnx for ONNX Runtime inference...\n",
      "image 1/1 /root/autodl-tmp/yolov5/data/images/bus.jpg: 640x640 4 persons, 1 bus, 1 handbag, Done. (0.014s)\n",
      "Speed: 3.0ms pre-process, 13.6ms inference, 4.9ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/onnx3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights ./runs/train/exp/weights/best.onnx --source ./data/images/bus.jpg --name onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5202a-e5c1-4399-b988-d86d7d450e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
